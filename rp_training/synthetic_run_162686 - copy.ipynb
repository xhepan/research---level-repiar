{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "670a94c4-70fb-4ad4-9a09-88a8d1d823c2",
   "metadata": {},
   "source": [
    "## Synthetic pair generation (new)\n",
    "Create **brand-new** valid-ish levels, corrupt them, and export JSONL pairs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08ceac94-3f60-4297-94b7-adc5d50d26c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T15:21:50.673617Z",
     "iopub.status.busy": "2025-10-10T15:21:50.673209Z",
     "iopub.status.idle": "2025-10-10T15:21:51.313760Z",
     "shell.execute_reply": "2025-10-10T15:21:51.312573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 200 pairs -> data/train_pairs.jsonl\n",
      "Wrote 40 pairs -> data/val_pairs.jsonl\n",
      "Synthetic pair generation complete.\n"
     ]
    }
   ],
   "source": [
    "import json, random\n",
    "from pathlib import Path\n",
    "\n",
    "random.seed(17)\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "N_TRAIN, N_VAL = 200, 40\n",
    "W, H = 64, 16\n",
    "OUT_DIR = Path(\"data\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_TRAIN = OUT_DIR / \"train_pairs.jsonl\"\n",
    "OUT_VAL   = OUT_DIR / \"val_pairs.jsonl\"\n",
    "\n",
    "# Token vocabulary\n",
    "VOCAB = ['M','F','y','Y','E','g','G','k','K','r','X','#','%','|','*','B','b','?','@','Q','!','1','2','D','S','C','U','L','o','t','T','<','>','[',']']\n",
    "BACKGROUND = '|'\n",
    "\n",
    "def blank_level():\n",
    "    return [[BACKGROUND for _ in range(W)] for _ in range(H)]\n",
    "\n",
    "def add_ground(level, min_h=1):\n",
    "    for y in range(H-1, H-1-min_h, -1):\n",
    "        for x in range(W):\n",
    "            level[y][x] = 'X'\n",
    "\n",
    "def add_platforms(level, n=8):\n",
    "    import random\n",
    "    for _ in range(n):\n",
    "        y = random.randint(5, H-4)\n",
    "        x0 = random.randint(2, W-8)\n",
    "        length = random.randint(3, 8)\n",
    "        for x in range(x0, min(W-1, x0+length)):\n",
    "            level[y][x] = 'S'\n",
    "\n",
    "def add_coins(level, n=60):\n",
    "    import random\n",
    "    for _ in range(n):\n",
    "        x = random.randint(1, W-2)\n",
    "        y = random.randint(3, H-5)\n",
    "        if level[y][x] == BACKGROUND:\n",
    "            level[y][x] = 'o'\n",
    "\n",
    "def add_enemies(level, n=20):\n",
    "    import random\n",
    "    for _ in range(n):\n",
    "        x = random.randint(2, W-3)\n",
    "        y = H-2\n",
    "        while y > 1 and level[y+1][x] == BACKGROUND:\n",
    "            y += 1\n",
    "        if level[y][x] in (BACKGROUND, 'o'):\n",
    "            level[y][x] = random.choice(['E','g','G','k','K','r'])\n",
    "\n",
    "def add_pipe(level):\n",
    "    import random\n",
    "    x0 = random.randint(6, W-8)\n",
    "    ground_y = H-2\n",
    "    ph = random.randint(2, 4)\n",
    "    for dy in range(ph):\n",
    "        level[ground_y - dy][x0]   = '['\n",
    "        level[ground_y - dy][x0+1] = ']'\n",
    "    level[ground_y - ph][x0]   = '<'\n",
    "    level[ground_y - ph][x0+1] = '>'\n",
    "    if random.random() < 0.2:\n",
    "        level[ground_y - ph - 1][x0] = 'T'\n",
    "\n",
    "def place_M_and_F(level):\n",
    "    import random\n",
    "    mx = random.randint(1, 4)\n",
    "    my = H-2\n",
    "    while my > 0 and level[my+1][mx] == BACKGROUND:\n",
    "        my += 1\n",
    "    level[my][mx] = 'M'\n",
    "    fx = random.randint(W-6, W-3)\n",
    "    fy = H-2\n",
    "    while fy > 0 and level[fy+1][fx] == BACKGROUND:\n",
    "        fy += 1\n",
    "    level[fy][fx] = 'F'\n",
    "\n",
    "def gen_clean_level():\n",
    "    lvl = blank_level()\n",
    "    add_ground(lvl, min_h=1)\n",
    "    add_platforms(lvl, n=random.randint(6, 10))\n",
    "    add_coins(lvl, n=random.randint(40, 80))\n",
    "    add_enemies(lvl, n=random.randint(10, 25))\n",
    "    for _ in range(random.randint(1, 3)):\n",
    "        add_pipe(lvl)\n",
    "    place_M_and_F(lvl)\n",
    "    return lvl\n",
    "\n",
    "def flatten_tokens(level):\n",
    "    return \" \".join(level[y][x] for y in range(H) for x in range(W))\n",
    "\n",
    "def corrupt_tokens(tokens, drop_p=0.02, flip_p=0.05, insert_p=0.01, remove_M_prob=0.25, remove_F_prob=0.20):\n",
    "    import random\n",
    "    out = []\n",
    "    for t in tokens:\n",
    "        r = random.random()\n",
    "        if t == 'M' and random.random() < remove_M_prob:\n",
    "            continue\n",
    "        if t == 'F' and random.random() < remove_F_prob:\n",
    "            continue\n",
    "        if r < drop_p:\n",
    "            continue\n",
    "        elif r < drop_p + flip_p:\n",
    "            out.append(random.choice(VOCAB))\n",
    "        else:\n",
    "            out.append(t)\n",
    "            if random.random() < insert_p:\n",
    "                out.append(random.choice(VOCAB))\n",
    "    return out or tokens\n",
    "\n",
    "def gen_pair():\n",
    "    clean = gen_clean_level()\n",
    "    clean_tokens = flatten_tokens(clean).split()\n",
    "    corrupted = corrupt_tokens(clean_tokens)\n",
    "    return \" \".join(corrupted), \" \".join(clean_tokens)\n",
    "\n",
    "def write_pairs(n, path):\n",
    "    with path.open(\"w\") as f:\n",
    "        for _ in range(n):\n",
    "            corr, clean = gen_pair()\n",
    "            f.write(json.dumps({\"corrupted\": corr, \"repaired\": clean}) + \"\\n\")\n",
    "    print(f\"Wrote {n} pairs -> {path}\")\n",
    "\n",
    "write_pairs(N_TRAIN, OUT_TRAIN)\n",
    "write_pairs(N_VAL,   OUT_VAL)\n",
    "print(\"Synthetic pair generation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e411f23",
   "metadata": {},
   "source": [
    "## Training (compat)\n",
    "Version-adaptive training cell that works across older/newer `transformers` installs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "335314e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T15:21:51.318050Z",
     "iopub.status.busy": "2025-10-10T15:21:51.317636Z",
     "iopub.status.idle": "2025-10-10T16:09:17.047877Z",
     "shell.execute_reply": "2025-10-10T16:09:17.046608Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home-mscluster/tmodukanelo/miniconda3/envs/rp_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.56.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 0 examples [00:00, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 200 examples [00:00, 658.90 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 200 examples [00:00, 437.11 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 602.76 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 512.50 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 0 examples [00:00, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Generating train split: 40 examples [00:00, 558.65 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 40/40 [00:00<00:00, 256.08 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 615.67 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 323.91 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 40/40 [00:00<00:00, 310.66 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Map: 100%|██████████| 40/40 [00:00<00:00, 151.00 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home-mscluster/tmodukanelo/miniconda3/envs/rp_env/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/home-mscluster/tmodukanelo/miniconda3/envs/rp_env/lib/python3.12/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 39:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.734700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running post-hoc evaluation (no evaluation_strategy supported by this version)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval metrics: {'eval_loss': 0.6493461728096008, 'eval_runtime': 43.4484, 'eval_samples_per_second': 0.921, 'eval_steps_per_second': 0.921, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (compat) finished. Saved to: out/llm-sft-pairs-compat\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer, DataCollatorForLanguageModeling,\n",
    "    __version__ as TR_VER\n",
    ")\n",
    "from inspect import signature\n",
    "\n",
    "print(\"Transformers version:\", TR_VER)\n",
    "\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "OUTPUT_DIR = \"out/llm-sft-pairs-compat\"\n",
    "\n",
    "train_file = \"data/train_pairs.jsonl\"\n",
    "val_file   = \"data/val_pairs.jsonl\"\n",
    "\n",
    "INSTR_FIELD = \"corrupted\"\n",
    "RESP_FIELD  = \"repaired\"\n",
    "BLOCK_SIZE  = 512\n",
    "BATCH_TOKENS = 4096\n",
    "\n",
    "def to_text(example):\n",
    "    instr = str(example[INSTR_FIELD]).strip()\n",
    "    resp  = str(example[RESP_FIELD]).strip()\n",
    "    example[\"text\"] = f\"<s>Instruction:\\\\n{instr}\\\\n\\\\nResponse:\\\\n{resp}</s>\"\n",
    "    return example\n",
    "\n",
    "train_ds = load_dataset(\"json\", data_files=train_file, split=\"train\").map(to_text)\n",
    "val_ds   = load_dataset(\"json\", data_files=val_file,   split=\"train\").map(to_text)\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "def tok_fn(examples):\n",
    "    return tok(examples[\"text\"], truncation=True, max_length=BLOCK_SIZE)\n",
    "\n",
    "train_tok = train_ds.map(tok_fn, batched=True, remove_columns=[\"text\"])\n",
    "val_tok   = val_ds.map(tok_fn,   batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n",
    "\n",
    "per_device_train_batch_size = 1\n",
    "gradient_accumulation_steps = max(1, BATCH_TOKENS // (per_device_train_batch_size * BLOCK_SIZE))\n",
    "per_device_eval_batch_size  = 1\n",
    "\n",
    "sig = signature(TrainingArguments.__init__).parameters\n",
    "def supports(name): return name in sig\n",
    "\n",
    "kwargs = dict(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_steps=1000,\n",
    "    eval_steps=500,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Batch size args (per_device vs per_gpu)\n",
    "if supports(\"per_device_train_batch_size\"):\n",
    "    kwargs[\"per_device_train_batch_size\"] = per_device_train_batch_size\n",
    "    kwargs[\"per_device_eval_batch_size\"] = per_device_eval_batch_size\n",
    "elif supports(\"per_gpu_train_batch_size\"):\n",
    "    kwargs[\"per_gpu_train_batch_size\"] = per_device_train_batch_size\n",
    "    if supports(\"per_gpu_eval_batch_size\"):\n",
    "        kwargs[\"per_gpu_eval_batch_size\"] = per_device_eval_batch_size\n",
    "\n",
    "# Scheduler + reporting\n",
    "if supports(\"lr_scheduler_type\"): kwargs[\"lr_scheduler_type\"] = \"cosine\"\n",
    "if supports(\"save_total_limit\"):  kwargs[\"save_total_limit\"] = 2\n",
    "if supports(\"report_to\"):         kwargs[\"report_to\"] = \"none\"\n",
    "\n",
    "# Evaluation control\n",
    "set_eval_after = False\n",
    "if supports(\"evaluation_strategy\"):\n",
    "    kwargs[\"evaluation_strategy\"] = \"steps\"\n",
    "elif supports(\"evaluate_during_training\"):\n",
    "    kwargs[\"evaluate_during_training\"] = True\n",
    "else:\n",
    "    # We'll evaluate explicitly after training\n",
    "    set_eval_after = True\n",
    "\n",
    "# Save strategy if available\n",
    "if supports(\"save_strategy\"):\n",
    "    kwargs[\"save_strategy\"] = \"steps\"\n",
    "\n",
    "args = TrainingArguments(**kwargs)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "if set_eval_after:\n",
    "    print(\"Running post-hoc evaluation (no evaluation_strategy supported by this version)...\")\n",
    "    metrics = trainer.evaluate(eval_dataset=val_tok)\n",
    "    print(\"Eval metrics:\", metrics)\n",
    "\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tok.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"Training (compat) finished. Saved to:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98fb9bdb-8bc5-46c1-a0a1-8964673c6ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: out/llm-sft-pairs-compat\n",
      "Running quick inline test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xhepon\\AppData\\Local\\Temp\\ipykernel_31264\\1203885245.py:68: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"meta\": {\"model_dir\": str(MODEL_DIR), \"ts\": datetime.utcnow().isoformat()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 2 repairs -> outputs\\repairs_quick.jsonl\n",
      "Done. Open outputs/repairs.jsonl (and repairs_quick.jsonl) to view results.\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Inference / Testing (corrupted → repaired)\n",
    "# Loads the fine-tuned model, formats the prompt exactly like training,\n",
    "# generates a repair for each corrupted input, and writes outputs to JSONL.\n",
    "\n",
    "# %% [code]\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "# Set this to the folder where you saved your fine-tuned model:\n",
    "MODEL_DIR = \"out/llm-sft-pairs-compat\"   # or: out/llm-sft-pairs / out/llm-sft-v2-pairs / out/llm-sft-v1\n",
    "assert Path(MODEL_DIR).exists(), f\"Model folder not found: {MODEL_DIR}\"\n",
    "\n",
    "RESULTS_DIR = Path(\"outputs\"); RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "IN_FILE  = Path(\"data/test_corrupted.jsonl\")   # optional: {\"corrupted\": \"...\"}\n",
    "OUT_FILE = RESULTS_DIR / \"repairs.jsonl\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Generation defaults (tweak to taste)\n",
    "GEN_KW = dict(\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    "    # num_beams=1,  # set >1 for beam search\n",
    ")\n",
    "\n",
    "# Match the training prompt template used earlier:\n",
    "def format_prompt(corrupted: str) -> str:\n",
    "    corrupted = corrupted.strip()\n",
    "    return f\"<s>Instruction:\\n{corrupted}\\n\\nResponse:\\n\"\n",
    "\n",
    "print(\"Loading model from:\", MODEL_DIR)\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_DIR)\n",
    "model.to(DEVICE).eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_repair(corrupted: str, gen_kwargs: Optional[Dict[str, Any]] = None) -> str:\n",
    "    prompt = format_prompt(corrupted)\n",
    "    enc = tok(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    gk = dict(GEN_KW)\n",
    "    if gen_kwargs: gk.update(gen_kwargs)\n",
    "    out = model.generate(**enc, **gk)\n",
    "    text = tok.decode(out[0], skip_special_tokens=True)\n",
    "    # Keep only the model's \"Response:\" part (the repaired level)\n",
    "    if \"Response:\" in text:\n",
    "        text = text.split(\"Response:\", 1)[1]\n",
    "    return text.strip()\n",
    "\n",
    "def run_batch(corrupted_list: List[str], outfile: Path = OUT_FILE) -> None:\n",
    "    from datetime import datetime\n",
    "    with outfile.open(\"w\", encoding=\"utf-8\") as g:\n",
    "        for c in corrupted_list:\n",
    "            rep = generate_repair(c)\n",
    "            rec = {\n",
    "                \"corrupted\": c,\n",
    "                \"repaired\": rep,\n",
    "                \"meta\": {\"model_dir\": str(MODEL_DIR), \"ts\": datetime.utcnow().isoformat()}\n",
    "            }\n",
    "            g.write(json.dumps(rec) + \"\\n\")\n",
    "    print(f\"Wrote {len(corrupted_list)} repairs -> {outfile}\")\n",
    "\n",
    "# ---------- Option A: JSONL file if available ----------\n",
    "if IN_FILE.exists():\n",
    "    data = []\n",
    "    with IN_FILE.open() as f:\n",
    "        for line in f:\n",
    "            ex = json.loads(line)\n",
    "            if \"corrupted\" in ex and ex[\"corrupted\"]:\n",
    "                data.append(str(ex[\"corrupted\"]))\n",
    "    if data:\n",
    "        print(f\"Found {len(data)} inputs in {IN_FILE}. Running batch inference...\")\n",
    "        run_batch(data, OUT_FILE)\n",
    "\n",
    "# ---------- Option B: Inline quick test ----------\n",
    "# Provide a few corrupted strings here to test quickly.\n",
    "# Replace with your own examples; tokens should match your training tokenization.\n",
    "quick_test = [\n",
    "    \"| | | | | | | | | | | | | | | b | | | | | | | | | | | | | | % | r M | | | | | | | ? | | | | | | | | | | | | | | | | | | | | | | | | | | | | | | [ | | | | | | | | | | | | | | | | | | | | | | | | K | | | | | | | | | | | | | | | | | | g | | | | | | | | | | | t | | | | | | | | | | | | | | | | @ | | | | | | | | | ! | | | | | | | | [ | | | | | K | | | | | | | | | | | | | b | | | | | | | | | | | | | o | | | | | | | | o | | o | | | S | o | | | | | | | | | | | [ | | | | | ] | | | | | | | o | | o | | | | | | o M | | | o | | | | | | | | o | o | | | | | | | | | | | | | | | | | | | | | | | | | o | | | | | | | | | o | | | | ? | | o | | | | | | | | | D | | | ] | | | | | | S S S S S S | 2 t | | S S S S | | | | | [ | o | | | | | | | | | | | | F | | | | | o | | | | | | | | | | | | | % o | | | | | | | | o | | X | | | | | | | | | | | | | | | T | | Y | | | | | | | | b | C | | | | | o | | | | | | | | | | | b | [ | | | S S S | | | | | | | | @ | o | | | o ? | | | | | | | o | | | | | | | U | | | | | | | o S S S S S S | | | | | | | | | | | | | o ! | | | o o | | 2 | | | ? | | | | | | | | | | | o o [ | | | | | | | | | | | | | | | | | | | | U o S | o b | o | | | | | | | | | | | | | | o | T | | | Q | 1 | | | | o o | | | | | | | o | | o | | | | | | | | o | U | | | | | | | o | | | | | | | | | | | | | | o | | | | | < o | | | | | | | | | | 2 | | | | | | | | | | | | o | | | | M | | | | | | | | o | o | | | | | o | | | | | 1 | | | | | | | | | | [ ] | | | | o | | | | | | | S S S S S S | | | | | o | | | | o > | | | | o | | | < > | | o | | | | | | | | | | | | | | | | | | [ ] | | B | | | | | | | | | | | | | | | | | | | | | S S S S S # S [ | | % | [ ] | | | | | | | | | | | | | | | | M | | | | | [ | | | | | | | | | | | | | | | | | * | | | | | | | | | | ? K | D | | | | | | | T ] | | | | | | | | | | | | C | t | | G g | g [ ] | r | | | | k | r | | r K | k | | | K | | G K k | | | g | | | 1 | | | | | [ ] g | g | | | | | X X X X X X X X X X X X X X X X X X X X S X X X X X X X X X X X X X X X X ! X X X X X X X X X X X X X X X X X X X X X X X > X\",\n",
    "    \"M | | X X X | | | [ ] < > | | o o | | | | | | | | X X X | | | F\",\n",
    "]\n",
    "\n",
    "if quick_test:\n",
    "    print(\"Running quick inline test...\")\n",
    "    run_batch(quick_test, RESULTS_DIR / \"repairs_quick.jsonl\")\n",
    "\n",
    "print(\"Done. Open outputs/repairs.jsonl (and repairs_quick.jsonl) to view results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79adfd9e-1162-4730-91a5-d801b85f3244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rp_mario)",
   "language": "python",
   "name": "rp_mario"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
