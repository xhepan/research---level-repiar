{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe149ef-a6f4-4ab7-82ac-229887e275a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processed 3843 level folders from: C:\\Users\\xhepon\\Documents\\a-No.2\\rp\\explore\\Mario-AI-Framework\\ship_to_train\\data\n",
      "Split -> Train: 3074 | Val: 384 | Test: 385  (seed=42)\n",
      "Outputs -> processed\\train.jsonl, processed\\val.jsonl, processed\\test.jsonl\n",
      "Saved   -> processed\\vocab.json\n",
      "Train avg dims (corrupted): 199.99x9.71 | (original): 199.99x8.08\n",
      "--- Cleaning report ---\n",
      "STRICT=False, UNKNOWN_POLICY='map_to_background'\n",
      "Note: counts reflect both corrupted/original files across all folders.\n",
      "folders_seen: 3843\n",
      "skipped_missing_files: 0\n",
      "skipped_comment_lines: 0\n",
      "skipped_separator_lines: 54759\n",
      "unknown_chars: 9928048\n",
      "dropped_chars: 0\n",
      "dropped_empty_after_clean: 0\n",
      "If '-' or other stray characters are meaningful tiles for you, tell me and I'll add them to VOCAB.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Build tokenized train/val/test JSONL from \"data\" folder (robust to stray chars/separators)\n",
    "\n",
    "from pathlib import Path\n",
    "import json, random, re\n",
    "from typing import List, Tuple, Dict, Iterable\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "DATA_DIR = Path(\"data\")            # root folder containing subfolders (one per level)\n",
    "OUT_DIR  = Path(\"processed\")       # where to write jsonl outputs\n",
    "SEED     = 42\n",
    "SPLITS   = (0.80, 0.10, 0.10)\n",
    "PAD_TO_RECTANGLE = True\n",
    "\n",
    "# Cleaning/sanitization behavior\n",
    "STRICT = False                     # if True -> error on unknown token; if False -> clean with policy below\n",
    "UNKNOWN_POLICY = \"map_to_background\"  # \"map_to_background\" | \"drop\"\n",
    "COMMENT_PREFIXES = (\"#\", \"//\", \";\")   # whole-line comments to skip\n",
    "SKIP_SEPARATOR_LINES = True           # skip lines made of 1 non-vocab char repeated (----, =====, etc.)\n",
    "MIN_SEP_RUN = 5\n",
    "\n",
    "# ----------------------------\n",
    "# Token vocabulary (from our project context)\n",
    "# ----------------------------\n",
    "VOCAB = ['M','F','y','Y','E','g','G','k','K','r','X','#','%','|','*','B','b','?','@','Q','!','1','2','D','S','C','U','L','o','t','T','<','>','[',']']\n",
    "BACKGROUND = '|'\n",
    "tok2id = {t:i for i,t in enumerate(VOCAB)}\n",
    "id2tok = {i:t for t,i in tok2id.items()}\n",
    "VOCAB_SET = set(VOCAB)\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def read_level_txt(p: Path) -> List[str]:\n",
    "    \"\"\"Return list of raw lines without trailing newlines; strip BOM; remove trailing empties.\"\"\"\n",
    "    with p.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        raw = f.read()\n",
    "    raw = raw.lstrip(\"\\ufeff\")  # strip BOM if present\n",
    "    lines = [ln.rstrip(\"\\n\\r\") for ln in raw.splitlines()]\n",
    "    while lines and lines[-1] == \"\":\n",
    "        lines.pop()\n",
    "    return lines\n",
    "\n",
    "SEP_LINE_RE = re.compile(r\"^(.)\\1+$\")  # same char repeated\n",
    "\n",
    "def is_separator_line(line: str) -> bool:\n",
    "    if not SKIP_SEPARATOR_LINES:\n",
    "        return False\n",
    "    if len(line) < MIN_SEP_RUN:\n",
    "        return False\n",
    "    m = SEP_LINE_RE.match(line)\n",
    "    if not m:\n",
    "        return False\n",
    "    ch = m.group(1)\n",
    "    return ch not in VOCAB_SET  # skip only if char is not a valid tile\n",
    "\n",
    "def sanitize_lines(lines: List[str], stats: Dict[str,int]) -> List[str]:\n",
    "    \"\"\"Remove comment/separator lines; optionally map/drop unknown chars. Update stats.\"\"\"\n",
    "    cleaned = []\n",
    "    for ln in lines:\n",
    "        striped = ln.strip()\n",
    "        if not striped:\n",
    "            continue\n",
    "        # whole-line comments\n",
    "        if any(striped.startswith(pref) for pref in COMMENT_PREFIXES):\n",
    "            stats[\"skipped_comment_lines\"] += 1\n",
    "            continue\n",
    "        # separator lines (-----, =====)\n",
    "        if is_separator_line(striped):\n",
    "            stats[\"skipped_separator_lines\"] += 1\n",
    "            continue\n",
    "\n",
    "        # character-level cleaning\n",
    "        new_chars = []\n",
    "        for ch in ln:\n",
    "            if ch in VOCAB_SET:\n",
    "                new_chars.append(ch)\n",
    "            else:\n",
    "                if STRICT:\n",
    "                    raise ValueError(f\"Unknown token '{ch}' in line: {ln}\")\n",
    "                stats[\"unknown_chars\"] += 1\n",
    "                if UNKNOWN_POLICY == \"map_to_background\":\n",
    "                    new_chars.append(BACKGROUND)\n",
    "                elif UNKNOWN_POLICY == \"drop\":\n",
    "                    # just skip this character\n",
    "                    stats[\"dropped_chars\"] += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    # fallback to mapping\n",
    "                    new_chars.append(BACKGROUND)\n",
    "        # keep line only if something remains (all-dropped lines vanish)\n",
    "        if new_chars:\n",
    "            cleaned.append(\"\".join(new_chars))\n",
    "        else:\n",
    "            stats[\"dropped_empty_after_clean\"] += 1\n",
    "    return cleaned\n",
    "\n",
    "def normalize_rectangular(lines: List[str]) -> Tuple[List[str], int, int]:\n",
    "    \"\"\"Pad all rows to the same width using BACKGROUND; return (lines, W, H).\"\"\"\n",
    "    if not lines:\n",
    "        return [], 0, 0\n",
    "    W = max(len(row) for row in lines)\n",
    "    H = len(lines)\n",
    "    if PAD_TO_RECTANGLE and W > 0:\n",
    "        lines = [row + (BACKGROUND * (W - len(row))) for row in lines]\n",
    "    return lines, W, H\n",
    "\n",
    "def tokenize_level(lines: List[str]) -> List[int]:\n",
    "    \"\"\"Flatten grid row-major into token IDs; assumes all chars ∈ VOCAB.\"\"\"\n",
    "    tokens: List[int] = []\n",
    "    for row in lines:\n",
    "        for ch in row:\n",
    "            tokens.append(tok2id[ch])\n",
    "    return tokens\n",
    "\n",
    "def collect_pairs(data_dir: Path):\n",
    "    \"\"\"Yield dicts with tokenized corrupted/original and basic shape info.\"\"\"\n",
    "    stats = {\n",
    "        \"folders_seen\": 0,\n",
    "        \"folders_kept\": 0,\n",
    "        \"skipped_missing_files\": 0,\n",
    "        \"skipped_comment_lines\": 0,\n",
    "        \"skipped_separator_lines\": 0,\n",
    "        \"unknown_chars\": 0,\n",
    "        \"dropped_chars\": 0,\n",
    "        \"dropped_empty_after_clean\": 0,\n",
    "        \"empty_after_sanitize_pairs\": 0,\n",
    "    }\n",
    "\n",
    "    for sub in sorted([p for p in data_dir.iterdir() if p.is_dir()]):\n",
    "        stats[\"folders_seen\"] += 1\n",
    "        corr = sub / \"corrupted.txt\"\n",
    "        orig = sub / \"original.txt\"\n",
    "        if not (corr.exists() and orig.exists()):\n",
    "            stats[\"skipped_missing_files\"] += 1\n",
    "            continue\n",
    "\n",
    "        corr_lines_raw = read_level_txt(corr)\n",
    "        orig_lines_raw = read_level_txt(orig)\n",
    "\n",
    "        corr_lines = sanitize_lines(corr_lines_raw, stats)\n",
    "        orig_lines = sanitize_lines(orig_lines_raw, stats)\n",
    "\n",
    "        # If sanitization nuked all rows, skip this pair\n",
    "        if not corr_lines or not orig_lines:\n",
    "            stats[\"empty_after_sanitize_pairs\"] += 1\n",
    "            continue\n",
    "\n",
    "        corr_lines, cW, cH = normalize_rectangular(corr_lines)\n",
    "        orig_lines, oW, oH = normalize_rectangular(orig_lines)\n",
    "\n",
    "        # Tokenize (now safe)\n",
    "        corr_ids = tokenize_level(corr_lines)\n",
    "        orig_ids = tokenize_level(orig_lines)\n",
    "\n",
    "        stats[\"folders_kept\"] += 1\n",
    "        yield {\n",
    "            \"level_id\": sub.name,\n",
    "            \"corrupted_ids\": corr_ids,\n",
    "            \"original_ids\": orig_ids,\n",
    "            \"width_corrupted\": cW,\n",
    "            \"height_corrupted\": cH,\n",
    "            \"width_original\": oW,\n",
    "            \"height_original\": oH,\n",
    "            # For debugging: uncomment if you want raw/clean text persisted\n",
    "            # \"corrupted_text\": \"\\n\".join(corr_lines),\n",
    "            # \"original_text\":  \"\\n\".join(orig_lines),\n",
    "        }\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# First pass: collect and also get cleaning stats\n",
    "pairs_iter = collect_pairs(DATA_DIR)\n",
    "pairs = list(pairs_iter)  # exhaust generator\n",
    "\n",
    "total = len(pairs)\n",
    "if total == 0:\n",
    "    raise RuntimeError(f\"No valid level pairs found under {DATA_DIR.resolve()} after sanitization. \"\n",
    "                       f\"Try setting STRICT=False and UNKNOWN_POLICY='map_to_background'.\")\n",
    "\n",
    "# Deterministic shuffle + split\n",
    "random.Random(SEED).shuffle(pairs)\n",
    "train_ratio, val_ratio, test_ratio = SPLITS\n",
    "assert abs((train_ratio + val_ratio + test_ratio) - 1.0) < 1e-9, \"SPLITS must sum to 1.0\"\n",
    "\n",
    "n_train = int(total * train_ratio)\n",
    "n_val   = int(total * val_ratio)\n",
    "n_test  = total - n_train - n_val\n",
    "\n",
    "train_set = pairs[:n_train]\n",
    "val_set   = pairs[n_train:n_train+n_val]\n",
    "test_set  = pairs[n_train+n_val:]\n",
    "\n",
    "def write_jsonl(path: Path, rows: Iterable[Dict]):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "write_jsonl(OUT_DIR / \"train.jsonl\", train_set)\n",
    "write_jsonl(OUT_DIR / \"val.jsonl\",   val_set)\n",
    "write_jsonl(OUT_DIR / \"test.jsonl\",  test_set)\n",
    "\n",
    "with (OUT_DIR / \"vocab.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"vocab\": VOCAB, \"tok2id\": tok2id}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ----------------------------\n",
    "# Summary (with cleaning report)\n",
    "# ----------------------------\n",
    "def avg_dims(rows, key_w, key_h):\n",
    "    if not rows: return (0.0, 0.0)\n",
    "    aw = sum(r.get(key_w, 0) for r in rows) / len(rows)\n",
    "    ah = sum(r.get(key_h, 0) for r in rows) / len(rows)\n",
    "    return (round(aw, 2), round(ah, 2))\n",
    "\n",
    "cW_tr, cH_tr = avg_dims(train_set, \"width_corrupted\", \"height_corrupted\")\n",
    "oW_tr, oH_tr = avg_dims(train_set, \"width_original\", \"height_original\")\n",
    "\n",
    "print(f\"✓ Processed {total} level folders from: {DATA_DIR.resolve()}\")\n",
    "print(f\"Split -> Train: {len(train_set)} | Val: {len(val_set)} | Test: {len(test_set)}  (seed={SEED})\")\n",
    "print(f\"Outputs -> {OUT_DIR / 'train.jsonl'}, {OUT_DIR / 'val.jsonl'}, {OUT_DIR / 'test.jsonl'}\")\n",
    "print(f\"Saved   -> {OUT_DIR / 'vocab.json'}\")\n",
    "print(f\"Train avg dims (corrupted): {cW_tr}x{cH_tr} | (original): {oW_tr}x{oH_tr}\")\n",
    "print(\"--- Cleaning report ---\")\n",
    "print(f\"STRICT={STRICT}, UNKNOWN_POLICY='{UNKNOWN_POLICY}'\")\n",
    "print(\"Note: counts reflect both corrupted/original files across all folders.\")\n",
    "# We can’t read the `stats` dict returned from a generator after exhaustion; re-scan quickly to print stats.\n",
    "# Light-weight rescan to only count issues:\n",
    "def quick_scan_unknowns(base: Path) -> Dict[str,int]:\n",
    "    s = {\"folders_seen\":0,\"skipped_missing_files\":0,\"skipped_comment_lines\":0,\n",
    "         \"skipped_separator_lines\":0,\"unknown_chars\":0,\"dropped_chars\":0,\"dropped_empty_after_clean\":0}\n",
    "    for sub in sorted([p for p in base.iterdir() if p.is_dir()]):\n",
    "        s[\"folders_seen\"] += 1\n",
    "        corr = sub / \"corrupted.txt\"\n",
    "        orig = sub / \"original.txt\"\n",
    "        if not (corr.exists() and orig.exists()):\n",
    "            s[\"skipped_missing_files\"] += 1\n",
    "            continue\n",
    "        for p in (corr, orig):\n",
    "            lines = read_level_txt(p)\n",
    "            # simulate sanitize (no errors)\n",
    "            _stats = {k:0 for k in s.keys()}\n",
    "            sanitize_lines(lines, _stats)\n",
    "            for k in s.keys():\n",
    "                if k in _stats: s[k] += _stats[k]\n",
    "    return s\n",
    "\n",
    "scan = quick_scan_unknowns(DATA_DIR)\n",
    "for k,v in scan.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "print(\"If '-' or other stray characters are meaningful tiles for you, tell me and I'll add them to VOCAB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f145909-39f2-4f07-867a-3e276e24ff21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rp_mario)",
   "language": "python",
   "name": "rp_mario"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
