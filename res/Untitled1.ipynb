{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcd54323-e28d-451e-82f8-e60e52f951ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n requires the protobuf library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\rp_mario\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2316\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2316\u001b[39m     tokenizer = \u001b[38;5;28mcls\u001b[39m(*init_inputs, **init_kwargs)\n\u001b[32m   2317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\rp_mario\\Lib\\site-packages\\transformers\\models\\qwen2\\tokenization_qwen2.py:172\u001b[39m, in \u001b[36mQwen2Tokenizer.__init__\u001b[39m\u001b[34m(self, vocab_file, merges_file, errors, unk_token, bos_token, eos_token, pad_token, clean_up_tokenization_spaces, split_special_tokens, **kwargs)\u001b[39m\n\u001b[32m    166\u001b[39m pad_token = (\n\u001b[32m    167\u001b[39m     AddedToken(pad_token, lstrip=\u001b[38;5;28;01mFalse\u001b[39;00m, rstrip=\u001b[38;5;28;01mFalse\u001b[39;00m, special=\u001b[38;5;28;01mTrue\u001b[39;00m, normalized=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pad_token, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m pad_token\n\u001b[32m    170\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(vocab_file, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m vocab_handle:\n\u001b[32m    173\u001b[39m     \u001b[38;5;28mself\u001b[39m.encoder = json.load(vocab_handle)\n",
      "\u001b[31mTypeError\u001b[39m: expected str, bytes or os.PathLike object, not NoneType",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# Load model/tokenizer\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m     92\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer.pad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     95\u001b[39m     tokenizer.pad_token = tokenizer.eos_token\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\rp_mario\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1144\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1141\u001b[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[32m   1143\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n\u001b[32m   1145\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1146\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\rp_mario\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2070\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2067\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2068\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2070\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._from_pretrained(\n\u001b[32m   2071\u001b[39m     resolved_vocab_files,\n\u001b[32m   2072\u001b[39m     pretrained_model_name_or_path,\n\u001b[32m   2073\u001b[39m     init_configuration,\n\u001b[32m   2074\u001b[39m     *init_inputs,\n\u001b[32m   2075\u001b[39m     token=token,\n\u001b[32m   2076\u001b[39m     cache_dir=cache_dir,\n\u001b[32m   2077\u001b[39m     local_files_only=local_files_only,\n\u001b[32m   2078\u001b[39m     _commit_hash=commit_hash,\n\u001b[32m   2079\u001b[39m     _is_local=is_local,\n\u001b[32m   2080\u001b[39m     trust_remote_code=trust_remote_code,\n\u001b[32m   2081\u001b[39m     **kwargs,\n\u001b[32m   2082\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\rp_mario\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2108\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2105\u001b[39m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[32m   2106\u001b[39m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[32m   2107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (from_slow \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_tokenizer_file) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.slow_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[32m-> \u001b[39m\u001b[32m2108\u001b[39m     slow_tokenizer = (\u001b[38;5;28mcls\u001b[39m.slow_tokenizer_class)._from_pretrained(\n\u001b[32m   2109\u001b[39m         copy.deepcopy(resolved_vocab_files),\n\u001b[32m   2110\u001b[39m         pretrained_model_name_or_path,\n\u001b[32m   2111\u001b[39m         copy.deepcopy(init_configuration),\n\u001b[32m   2112\u001b[39m         *init_inputs,\n\u001b[32m   2113\u001b[39m         token=token,\n\u001b[32m   2114\u001b[39m         cache_dir=cache_dir,\n\u001b[32m   2115\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   2116\u001b[39m         _commit_hash=_commit_hash,\n\u001b[32m   2117\u001b[39m         **(copy.deepcopy(kwargs)),\n\u001b[32m   2118\u001b[39m     )\n\u001b[32m   2119\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2120\u001b[39m     slow_tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\rp_mario\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2317\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2316\u001b[39m     tokenizer = \u001b[38;5;28mcls\u001b[39m(*init_inputs, **init_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[32m   2318\u001b[39m     logger.info(\n\u001b[32m   2319\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2320\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2321\u001b[39m     )\n\u001b[32m   2322\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\rp_mario\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:87\u001b[39m, in \u001b[36mimport_protobuf_decode_error\u001b[39m\u001b[34m(error_message)\u001b[39m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DecodeError\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PROTOBUF_IMPORT_ERROR.format(error_message))\n",
      "\u001b[31mImportError\u001b[39m: \n requires the protobuf library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Run the fine-tuned HF model to repair new corrupted levels in \"test_data\"\n",
    "# Assumes you fine-tuned with the earlier HF cell (Qwen/gpt-oss/etc.)\n",
    "# !pip install -q transformers accelerate\n",
    "\n",
    "import json, re, os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "MODEL_DIR = \"hf-checkpoints/level-repair-qwen/checkpoint-300\"   # <- change if you saved elsewhere\n",
    "TEST_DIR  = Path(\"test_data\")                     # folder of level folders each with corrupted.txt (and optionally metadata.json)\n",
    "OUT_DIR   = Path(\"repairs_out\")\n",
    "\n",
    "# generation settings\n",
    "MAX_NEW_TOKENS   = 4096\n",
    "DO_SAMPLE        = False   # greedy by default; set True for sampling\n",
    "TEMPERATURE      = 0.8\n",
    "TOP_P            = 0.95\n",
    "\n",
    "# sanitization behavior (mirrors training preprocess defaults)\n",
    "VOCAB = ['M','F','y','Y','E','g','G','k','K','r','X','#','%','|','*','B','b','?','@','Q','!','1','2','D','S','C','U','L','o','t','T','<','>','[',']']\n",
    "BACKGROUND = '|'\n",
    "VOCAB_SET = set(VOCAB)\n",
    "COMMENT_PREFIXES = (\"#\", \"//\", \";\")\n",
    "SKIP_SEPARATOR_LINES = True\n",
    "MIN_SEP_RUN = 5\n",
    "STRICT = False                # if True, error on unknown chars; else map to BACKGROUND\n",
    "UNKNOWN_POLICY = \"map_to_background\"  # or \"drop\"\n",
    "\n",
    "# If you saved this during preprocessing, we can use it to produce IDs.\n",
    "VOCAB_PATH = Path(\"processed/vocab.json\")  # optional\n",
    "tok2id = {t:i for i,t in enumerate(VOCAB)}\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def read_text(path: Path) -> List[str]:\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        raw = f.read().lstrip(\"\\ufeff\")\n",
    "    lines = [ln.rstrip(\"\\n\\r\") for ln in raw.splitlines()]\n",
    "    while lines and lines[-1] == \"\":\n",
    "        lines.pop()\n",
    "    return lines\n",
    "\n",
    "SEP_LINE_RE = re.compile(r\"^(.)\\1+$\")\n",
    "\n",
    "def is_separator_line(line: str) -> bool:\n",
    "    if not SKIP_SEPARATOR_LINES or len(line) < MIN_SEP_RUN: return False\n",
    "    m = SEP_LINE_RE.match(line)\n",
    "    if not m: return False\n",
    "    ch = m.group(1)\n",
    "    return ch not in VOCAB_SET\n",
    "\n",
    "def sanitize_lines(lines: List[str]) -> List[str]:\n",
    "    cleaned = []\n",
    "    for ln in lines:\n",
    "        s = ln.strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        if any(s.startswith(pref) for pref in COMMENT_PREFIXES):\n",
    "            continue\n",
    "        if is_separator_line(s):\n",
    "            continue\n",
    "        new_chars = []\n",
    "        for ch in ln:\n",
    "            if ch in VOCAB_SET:\n",
    "                new_chars.append(ch)\n",
    "            else:\n",
    "                if STRICT:\n",
    "                    raise ValueError(f\"Unknown token '{ch}' in line: {ln}\")\n",
    "                if UNKNOWN_POLICY == \"map_to_background\":\n",
    "                    new_chars.append(BACKGROUND)\n",
    "                elif UNKNOWN_POLICY == \"drop\":\n",
    "                    continue\n",
    "                else:\n",
    "                    new_chars.append(BACKGROUND)\n",
    "        if new_chars:\n",
    "            cleaned.append(\"\".join(new_chars))\n",
    "    return cleaned\n",
    "\n",
    "def to_ids(grid_text: str) -> List[int]:\n",
    "    return [tok2id.get(ch, tok2id[BACKGROUND]) for ch in grid_text.replace(\"\\n\", \"\")]\n",
    "\n",
    "# -----------------------------\n",
    "# Load model/tokenizer\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_DIR, trust_remote_code=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "INSTR = \"### Instruction:\\nRepair the level.\\n\\n\"\n",
    "CORR  = \"### Corrupted:\\n\"\n",
    "REPR  = \"\\n\\n### Repaired:\\n\"\n",
    "\n",
    "def build_prompt(corrupted_lines: List[str]) -> str:\n",
    "    corrupted_txt = \"\\n\".join(corrupted_lines)\n",
    "    return INSTR + CORR + corrupted_txt + REPR\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_repair(corrupted_lines: List[str]) -> str:\n",
    "    prompt = build_prompt(corrupted_lines)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    gen_kwargs = dict(\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=DO_SAMPLE,\n",
    "        temperature=TEMPERATURE,\n",
    "        top_p=TOP_P,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    out = model.generate(**inputs, **gen_kwargs)\n",
    "    gen_text = tokenizer.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    return gen_text\n",
    "\n",
    "# -----------------------------\n",
    "# Run over test_data\n",
    "# -----------------------------\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "summary_path = OUT_DIR / \"predictions.jsonl\"\n",
    "n_done = 0\n",
    "\n",
    "with summary_path.open(\"w\", encoding=\"utf-8\") as jf:\n",
    "    for level_dir in sorted([p for p in TEST_DIR.iterdir() if p.is_dir()]):\n",
    "        corr_path = level_dir / \"corrupted.txt\"\n",
    "        if not corr_path.exists():\n",
    "            continue\n",
    "\n",
    "        # sanitize -> prompt -> generate\n",
    "        raw_lines  = read_text(corr_path)\n",
    "        clean_lines = sanitize_lines(raw_lines)\n",
    "        if not clean_lines:\n",
    "            # skip empty after cleaning\n",
    "            continue\n",
    "\n",
    "        pred_txt = generate_repair(clean_lines)\n",
    "\n",
    "        # ensure an output subfolder per level_id\n",
    "        out_sub = OUT_DIR / level_dir.name\n",
    "        out_sub.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # save repaired grid text\n",
    "        repaired_path = out_sub / \"repaired.txt\"\n",
    "        with repaired_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(pred_txt.rstrip() + \"\\n\")\n",
    "\n",
    "        # optional: save predicted ids (flattened; newlines removed)\n",
    "        repaired_ids_path = out_sub / \"repaired_ids.json\"\n",
    "        try:\n",
    "            pred_ids = to_ids(pred_txt)\n",
    "            json.dump({\"level_id\": level_dir.name, \"repaired_ids\": pred_ids}, repaired_ids_path.open(\"w\", encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            # if mapping fails for any reason, skip ids\n",
    "            pass\n",
    "\n",
    "        # write to summary jsonl\n",
    "        rec = {\n",
    "            \"level_id\": level_dir.name,\n",
    "            \"corrupted_preview_first3\": clean_lines[:3],\n",
    "            \"repaired_preview_first3\": pred_txt.splitlines()[:3],\n",
    "            \"repaired_path\": str(repaired_path),\n",
    "        }\n",
    "        jf.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "        n_done += 1\n",
    "\n",
    "print(f\"✓ Repaired {n_done} levels from {TEST_DIR} -> {OUT_DIR}\")\n",
    "print(f\"Summary JSONL: {summary_path}\")\n",
    "print(\"Example preview (open any repaired.txt inside repairs_out/<level_id>/):\")\n",
    "for p in sorted(OUT_DIR.iterdir()):\n",
    "    if p.is_dir():\n",
    "        sample = p / \"repaired.txt\"\n",
    "        if sample.exists():\n",
    "            print(\" -\", p.name, \"->\", sample)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9d0ce8-fdaf-4dfe-97ae-75a4a37afa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers: 4.56.1\n",
      "Tokenizer not found in fine-tuned dir (local): ImportError('\\n requires the protobuf library but it was not found in your environment. Check out the instructions on the\\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\\nthat match your environment. Please note that you may need to restart your runtime after installation.\\n')\n",
      "Tokenizer not found in fine-tuned dir: ImportError('\\n requires the protobuf library but it was not found in your environment. Check out the instructions on the\\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\\nthat match your environment. Please note that you may need to restart your runtime after installation.\\n')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xhepon\\anaconda3\\envs\\rp_mario\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\xhepon\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-0.5B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from base model: Qwen/Qwen2.5-0.5B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from fine-tuned dir.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Robust inference for repairing levels with your fine-tuned HF model\n",
    "\n",
    "# If you see ImportError about protobuf / sentencepiece / tiktoken, install:\n",
    "# %pip install -U transformers accelerate\n",
    "# %pip install protobuf sentencepiece tiktoken\n",
    "\n",
    "import os, re, json, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "print(\"Transformers:\", transformers.__version__)\n",
    "\n",
    "# -----------------------------\n",
    "# Paths & config\n",
    "# -----------------------------\n",
    "MODEL_DIR = \"hf-checkpoints/level-repair-qwen/checkpoint-300\"     # your fine-tuned checkpoint dir\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"              \n",
    "TEST_DIR  = Path(\"test_data\")                      # each subfolder contains corrupted.txt\n",
    "OUT_DIR   = Path(\"repairs_out\")\n",
    "\n",
    "# Generation settings\n",
    "MAX_NEW_TOKENS = 4096\n",
    "DO_SAMPLE      = False\n",
    "TEMPERATURE    = 0.8\n",
    "TOP_P          = 0.95\n",
    "\n",
    "# Sanitization (match training preprocessing)\n",
    "VOCAB = ['M','F','y','Y','E','g','G','k','K','r','X','#','%','|','*','B','b','?','@','Q','!','1','2','D','S','C','U','L','o','t','T','<','>','[',']']\n",
    "BACKGROUND = '|'\n",
    "VOCAB_SET = set(VOCAB)\n",
    "COMMENT_PREFIXES = (\"#\", \"//\", \";\")\n",
    "MIN_SEP_RUN = 5\n",
    "STRICT = False\n",
    "UNKNOWN_POLICY = \"map_to_background\"  # or \"drop\"\n",
    "\n",
    "tok2id = {t:i for i,t in enumerate(VOCAB)}\n",
    "\n",
    "# -----------------------------\n",
    "# IO helpers\n",
    "# -----------------------------\n",
    "def read_text(path: Path) -> List[str]:\n",
    "    with path.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        raw = f.read().lstrip(\"\\ufeff\")\n",
    "    lines = [ln.rstrip(\"\\n\\r\") for ln in raw.splitlines()]\n",
    "    while lines and lines[-1] == \"\":\n",
    "        lines.pop()\n",
    "    return lines\n",
    "\n",
    "SEP_LINE_RE = re.compile(r\"^(.)\\1+$\")\n",
    "def is_separator_line(line: str) -> bool:\n",
    "    if len(line) < MIN_SEP_RUN: return False\n",
    "    m = SEP_LINE_RE.match(line)\n",
    "    if not m: return False\n",
    "    ch = m.group(1)\n",
    "    return ch not in VOCAB_SET\n",
    "\n",
    "def sanitize_lines(lines: List[str]) -> List[str]:\n",
    "    cleaned = []\n",
    "    for ln in lines:\n",
    "        s = ln.strip()\n",
    "        if not s: continue\n",
    "        if any(s.startswith(pref) for pref in COMMENT_PREFIXES): continue\n",
    "        if is_separator_line(s): continue\n",
    "        row = []\n",
    "        for ch in ln:\n",
    "            if ch in VOCAB_SET:\n",
    "                row.append(ch)\n",
    "            else:\n",
    "                if STRICT:\n",
    "                    raise ValueError(f\"Unknown token '{ch}' in line: {ln}\")\n",
    "                if UNKNOWN_POLICY == \"drop\":\n",
    "                    continue\n",
    "                row.append(BACKGROUND)\n",
    "        if row:\n",
    "            cleaned.append(\"\".join(row))\n",
    "    return cleaned\n",
    "\n",
    "def to_ids(grid_text: str) -> List[int]:\n",
    "    return [tok2id.get(ch, tok2id[BACKGROUND]) for ch in grid_text.replace(\"\\n\", \"\")]\n",
    "\n",
    "# -----------------------------\n",
    "# Load tokenizer & model (robust)\n",
    "# -----------------------------\n",
    "def load_tokenizer_with_fallback(finetuned_dir: str, base_model: str):\n",
    "    # Try local FT dir first (fast, offline)\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(finetuned_dir, trust_remote_code=True, local_files_only=True)\n",
    "        print(\"Tokenizer loaded from fine-tuned dir (local).\")\n",
    "        return tok\n",
    "    except Exception as e_local:\n",
    "        print(\"Tokenizer not found in fine-tuned dir (local):\", repr(e_local))\n",
    "\n",
    "    # Try FT dir allowing downloads (if any missing files)\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(finetuned_dir, trust_remote_code=True)\n",
    "        print(\"Tokenizer loaded from fine-tuned dir.\")\n",
    "        return tok\n",
    "    except Exception as e_ft:\n",
    "        print(\"Tokenizer not found in fine-tuned dir:\", repr(e_ft))\n",
    "\n",
    "    # Fall back to base model tokenizer\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "        print(f\"Tokenizer loaded from base model: {base_model}\")\n",
    "        return tok\n",
    "    except Exception as e_base:\n",
    "        print(\"Base tokenizer load failed:\", repr(e_base))\n",
    "        raise RuntimeError(\n",
    "            \"Failed to load a tokenizer. If the error mentions protobuf or sentencepiece, run:\\n\"\n",
    "            \"  pip install protobuf sentencepiece tiktoken\\n\"\n",
    "            \"Also ensure BASE_MODEL_NAME matches the model you fine-tuned.\"\n",
    "        )\n",
    "\n",
    "def load_model_finetuned(finetuned_dir: str):\n",
    "    # We expect weights to be in the fine-tuned dir\n",
    "    return AutoModelForCausalLM.from_pretrained(finetuned_dir, trust_remote_code=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = load_tokenizer_with_fallback(MODEL_DIR, BASE_MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    # Many decoder-only models lack a pad; use eos\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "try:\n",
    "    model = load_model_finetuned(MODEL_DIR).to(device)\n",
    "    print(\"Model loaded from fine-tuned dir.\")\n",
    "except Exception as e_model:\n",
    "    raise RuntimeError(\n",
    "        f\"Could not load model from {MODEL_DIR}. Ensure trainer.save_model(...) wrote weights there.\\n\"\n",
    "        f\"Original error: {e_model}\"\n",
    "    )\n",
    "\n",
    "model.eval()\n",
    "\n",
    "INSTR = \"### Instruction:\\nRepair the level.\\n\\n\"\n",
    "CORR  = \"### Corrupted:\\n\"\n",
    "REPR  = \"\\n\\n### Repaired:\\n\"\n",
    "\n",
    "def build_prompt(corrupted_lines: List[str]) -> str:\n",
    "    return INSTR + CORR + \"\\n\".join(corrupted_lines) + REPR\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_repair(corrupted_lines: List[str]) -> str:\n",
    "    prompt = build_prompt(corrupted_lines)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=DO_SAMPLE,\n",
    "        temperature=TEMPERATURE,\n",
    "        top_p=TOP_P,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    return tokenizer.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Run over test_data\n",
    "# -----------------------------\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "summary_path = OUT_DIR / \"predictions.jsonl\"\n",
    "n_done = 0\n",
    "\n",
    "with summary_path.open(\"w\", encoding=\"utf-8\") as jf:\n",
    "    for level_dir in sorted([p for p in TEST_DIR.iterdir() if p.is_dir()]):\n",
    "        corr = level_dir / \"corrupted.txt\"\n",
    "        if not corr.exists():\n",
    "            continue\n",
    "        raw = read_text(corr)\n",
    "        clean = sanitize_lines(raw)\n",
    "        if not clean:\n",
    "            continue\n",
    "\n",
    "        pred_txt = generate_repair(clean)\n",
    "\n",
    "        sub_out = OUT_DIR / level_dir.name\n",
    "        sub_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with (sub_out / \"repaired.txt\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(pred_txt.rstrip() + \"\\n\")\n",
    "\n",
    "        # Optional ID export\n",
    "        try:\n",
    "            pred_ids = to_ids(pred_txt)\n",
    "            with (sub_out / \"repaired_ids.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump({\"level_id\": level_dir.name, \"repaired_ids\": pred_ids}, f)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        rec = {\n",
    "            \"level_id\": level_dir.name,\n",
    "            \"repaired_path\": str(sub_out / \"repaired.txt\"),\n",
    "            \"corrupted_preview_first3\": clean[:3],\n",
    "            \"repaired_preview_first3\": pred_txt.splitlines()[:3],\n",
    "        }\n",
    "        jf.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "        n_done += 1\n",
    "\n",
    "print(f\"✓ Repaired {n_done} levels from {TEST_DIR} -> {OUT_DIR}\")\n",
    "print(f\"Summary: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d680f265-e8ad-444f-8872-71ef508fb093",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rp_mario)",
   "language": "python",
   "name": "rp_mario"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
