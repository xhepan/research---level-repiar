{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe149ef-a6f4-4ab7-82ac-229887e275a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Processed 3843 level folders from: C:\\Users\\xhepon\\Documents\\a-No.2\\rp\\explore\\Mario-AI-Framework\\ship_to_train\\data\n",
      "Split -> Train: 3074 | Val: 384 | Test: 385  (seed=42)\n",
      "Outputs -> processed\\train.jsonl, processed\\val.jsonl, processed\\test.jsonl\n",
      "Saved   -> processed\\vocab.json\n",
      "Train avg dims (corrupted): 199.99x9.71 | (original): 199.99x8.08\n",
      "--- Cleaning report ---\n",
      "STRICT=False, UNKNOWN_POLICY='map_to_background'\n",
      "Note: counts reflect both corrupted/original files across all folders.\n",
      "folders_seen: 3843\n",
      "skipped_missing_files: 0\n",
      "skipped_comment_lines: 0\n",
      "skipped_separator_lines: 54759\n",
      "unknown_chars: 9928048\n",
      "dropped_chars: 0\n",
      "dropped_empty_after_clean: 0\n",
      "If '-' or other stray characters are meaningful tiles for you, tell me and I'll add them to VOCAB.\n"
     ]
    }
   ],
   "source": [
    "# Cell: Build tokenized train/val/test JSONL from \"data\" folder (robust to stray chars/separators)\n",
    "\n",
    "from pathlib import Path\n",
    "import json, random, re\n",
    "from typing import List, Tuple, Dict, Iterable\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "DATA_DIR = Path(\"data\")            # root folder containing subfolders (one per level)\n",
    "OUT_DIR  = Path(\"processed\")       # where to write jsonl outputs\n",
    "SEED     = 42\n",
    "SPLITS   = (0.80, 0.10, 0.10)\n",
    "PAD_TO_RECTANGLE = True\n",
    "\n",
    "# Cleaning/sanitization behavior\n",
    "STRICT = False                     # if True -> error on unknown token; if False -> clean with policy below\n",
    "UNKNOWN_POLICY = \"map_to_background\"  # \"map_to_background\" | \"drop\"\n",
    "COMMENT_PREFIXES = (\"#\", \"//\", \";\")   # whole-line comments to skip\n",
    "SKIP_SEPARATOR_LINES = True           # skip lines made of 1 non-vocab char repeated (----, =====, etc.)\n",
    "MIN_SEP_RUN = 5\n",
    "\n",
    "# ----------------------------\n",
    "# Token vocabulary (from our project context)\n",
    "# ----------------------------\n",
    "VOCAB = ['M','F','y','Y','E','g','G','k','K','r','X','#','%','|','*','B','b','?','@','Q','!','1','2','D','S','C','U','L','o','t','T','<','>','[',']']\n",
    "BACKGROUND = '|'\n",
    "tok2id = {t:i for i,t in enumerate(VOCAB)}\n",
    "id2tok = {i:t for t,i in tok2id.items()}\n",
    "VOCAB_SET = set(VOCAB)\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def read_level_txt(p: Path) -> List[str]:\n",
    "    \"\"\"Return list of raw lines without trailing newlines; strip BOM; remove trailing empties.\"\"\"\n",
    "    with p.open(\"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        raw = f.read()\n",
    "    raw = raw.lstrip(\"\\ufeff\")  # strip BOM if present\n",
    "    lines = [ln.rstrip(\"\\n\\r\") for ln in raw.splitlines()]\n",
    "    while lines and lines[-1] == \"\":\n",
    "        lines.pop()\n",
    "    return lines\n",
    "\n",
    "SEP_LINE_RE = re.compile(r\"^(.)\\1+$\")  # same char repeated\n",
    "\n",
    "def is_separator_line(line: str) -> bool:\n",
    "    if not SKIP_SEPARATOR_LINES:\n",
    "        return False\n",
    "    if len(line) < MIN_SEP_RUN:\n",
    "        return False\n",
    "    m = SEP_LINE_RE.match(line)\n",
    "    if not m:\n",
    "        return False\n",
    "    ch = m.group(1)\n",
    "    return ch not in VOCAB_SET  # skip only if char is not a valid tile\n",
    "\n",
    "def sanitize_lines(lines: List[str], stats: Dict[str,int]) -> List[str]:\n",
    "    \"\"\"Remove comment/separator lines; optionally map/drop unknown chars. Update stats.\"\"\"\n",
    "    cleaned = []\n",
    "    for ln in lines:\n",
    "        striped = ln.strip()\n",
    "        if not striped:\n",
    "            continue\n",
    "        # whole-line comments\n",
    "        if any(striped.startswith(pref) for pref in COMMENT_PREFIXES):\n",
    "            stats[\"skipped_comment_lines\"] += 1\n",
    "            continue\n",
    "        # separator lines (-----, =====)\n",
    "        if is_separator_line(striped):\n",
    "            stats[\"skipped_separator_lines\"] += 1\n",
    "            continue\n",
    "\n",
    "        # character-level cleaning\n",
    "        new_chars = []\n",
    "        for ch in ln:\n",
    "            if ch in VOCAB_SET:\n",
    "                new_chars.append(ch)\n",
    "            else:\n",
    "                if STRICT:\n",
    "                    raise ValueError(f\"Unknown token '{ch}' in line: {ln}\")\n",
    "                stats[\"unknown_chars\"] += 1\n",
    "                if UNKNOWN_POLICY == \"map_to_background\":\n",
    "                    new_chars.append(BACKGROUND)\n",
    "                elif UNKNOWN_POLICY == \"drop\":\n",
    "                    # just skip this character\n",
    "                    stats[\"dropped_chars\"] += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    # fallback to mapping\n",
    "                    new_chars.append(BACKGROUND)\n",
    "        # keep line only if something remains (all-dropped lines vanish)\n",
    "        if new_chars:\n",
    "            cleaned.append(\"\".join(new_chars))\n",
    "        else:\n",
    "            stats[\"dropped_empty_after_clean\"] += 1\n",
    "    return cleaned\n",
    "\n",
    "def normalize_rectangular(lines: List[str]) -> Tuple[List[str], int, int]:\n",
    "    \"\"\"Pad all rows to the same width using BACKGROUND; return (lines, W, H).\"\"\"\n",
    "    if not lines:\n",
    "        return [], 0, 0\n",
    "    W = max(len(row) for row in lines)\n",
    "    H = len(lines)\n",
    "    if PAD_TO_RECTANGLE and W > 0:\n",
    "        lines = [row + (BACKGROUND * (W - len(row))) for row in lines]\n",
    "    return lines, W, H\n",
    "\n",
    "def tokenize_level(lines: List[str]) -> List[int]:\n",
    "    \"\"\"Flatten grid row-major into token IDs; assumes all chars ∈ VOCAB.\"\"\"\n",
    "    tokens: List[int] = []\n",
    "    for row in lines:\n",
    "        for ch in row:\n",
    "            tokens.append(tok2id[ch])\n",
    "    return tokens\n",
    "\n",
    "def collect_pairs(data_dir: Path):\n",
    "    \"\"\"Yield dicts with tokenized corrupted/original and basic shape info.\"\"\"\n",
    "    stats = {\n",
    "        \"folders_seen\": 0,\n",
    "        \"folders_kept\": 0,\n",
    "        \"skipped_missing_files\": 0,\n",
    "        \"skipped_comment_lines\": 0,\n",
    "        \"skipped_separator_lines\": 0,\n",
    "        \"unknown_chars\": 0,\n",
    "        \"dropped_chars\": 0,\n",
    "        \"dropped_empty_after_clean\": 0,\n",
    "        \"empty_after_sanitize_pairs\": 0,\n",
    "    }\n",
    "\n",
    "    for sub in sorted([p for p in data_dir.iterdir() if p.is_dir()]):\n",
    "        stats[\"folders_seen\"] += 1\n",
    "        corr = sub / \"corrupted.txt\"\n",
    "        orig = sub / \"original.txt\"\n",
    "        if not (corr.exists() and orig.exists()):\n",
    "            stats[\"skipped_missing_files\"] += 1\n",
    "            continue\n",
    "\n",
    "        corr_lines_raw = read_level_txt(corr)\n",
    "        orig_lines_raw = read_level_txt(orig)\n",
    "\n",
    "        corr_lines = sanitize_lines(corr_lines_raw, stats)\n",
    "        orig_lines = sanitize_lines(orig_lines_raw, stats)\n",
    "\n",
    "        # If sanitization nuked all rows, skip this pair\n",
    "        if not corr_lines or not orig_lines:\n",
    "            stats[\"empty_after_sanitize_pairs\"] += 1\n",
    "            continue\n",
    "\n",
    "        corr_lines, cW, cH = normalize_rectangular(corr_lines)\n",
    "        orig_lines, oW, oH = normalize_rectangular(orig_lines)\n",
    "\n",
    "        # Tokenize (now safe)\n",
    "        corr_ids = tokenize_level(corr_lines)\n",
    "        orig_ids = tokenize_level(orig_lines)\n",
    "\n",
    "        stats[\"folders_kept\"] += 1\n",
    "        yield {\n",
    "            \"level_id\": sub.name,\n",
    "            \"corrupted_ids\": corr_ids,\n",
    "            \"original_ids\": orig_ids,\n",
    "            \"width_corrupted\": cW,\n",
    "            \"height_corrupted\": cH,\n",
    "            \"width_original\": oW,\n",
    "            \"height_original\": oH,\n",
    "            # For debugging: uncomment if you want raw/clean text persisted\n",
    "            # \"corrupted_text\": \"\\n\".join(corr_lines),\n",
    "            # \"original_text\":  \"\\n\".join(orig_lines),\n",
    "        }\n",
    "\n",
    "    return stats\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# First pass: collect and also get cleaning stats\n",
    "pairs_iter = collect_pairs(DATA_DIR)\n",
    "pairs = list(pairs_iter)  # exhaust generator\n",
    "\n",
    "total = len(pairs)\n",
    "if total == 0:\n",
    "    raise RuntimeError(f\"No valid level pairs found under {DATA_DIR.resolve()} after sanitization. \"\n",
    "                       f\"Try setting STRICT=False and UNKNOWN_POLICY='map_to_background'.\")\n",
    "\n",
    "# Deterministic shuffle + split\n",
    "random.Random(SEED).shuffle(pairs)\n",
    "train_ratio, val_ratio, test_ratio = SPLITS\n",
    "assert abs((train_ratio + val_ratio + test_ratio) - 1.0) < 1e-9, \"SPLITS must sum to 1.0\"\n",
    "\n",
    "n_train = int(total * train_ratio)\n",
    "n_val   = int(total * val_ratio)\n",
    "n_test  = total - n_train - n_val\n",
    "\n",
    "train_set = pairs[:n_train]\n",
    "val_set   = pairs[n_train:n_train+n_val]\n",
    "test_set  = pairs[n_train+n_val:]\n",
    "\n",
    "def write_jsonl(path: Path, rows: Iterable[Dict]):\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "write_jsonl(OUT_DIR / \"train.jsonl\", train_set)\n",
    "write_jsonl(OUT_DIR / \"val.jsonl\",   val_set)\n",
    "write_jsonl(OUT_DIR / \"test.jsonl\",  test_set)\n",
    "\n",
    "with (OUT_DIR / \"vocab.json\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"vocab\": VOCAB, \"tok2id\": tok2id}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ----------------------------\n",
    "# Summary (with cleaning report)\n",
    "# ----------------------------\n",
    "def avg_dims(rows, key_w, key_h):\n",
    "    if not rows: return (0.0, 0.0)\n",
    "    aw = sum(r.get(key_w, 0) for r in rows) / len(rows)\n",
    "    ah = sum(r.get(key_h, 0) for r in rows) / len(rows)\n",
    "    return (round(aw, 2), round(ah, 2))\n",
    "\n",
    "cW_tr, cH_tr = avg_dims(train_set, \"width_corrupted\", \"height_corrupted\")\n",
    "oW_tr, oH_tr = avg_dims(train_set, \"width_original\", \"height_original\")\n",
    "\n",
    "print(f\"✓ Processed {total} level folders from: {DATA_DIR.resolve()}\")\n",
    "print(f\"Split -> Train: {len(train_set)} | Val: {len(val_set)} | Test: {len(test_set)}  (seed={SEED})\")\n",
    "print(f\"Outputs -> {OUT_DIR / 'train.jsonl'}, {OUT_DIR / 'val.jsonl'}, {OUT_DIR / 'test.jsonl'}\")\n",
    "print(f\"Saved   -> {OUT_DIR / 'vocab.json'}\")\n",
    "print(f\"Train avg dims (corrupted): {cW_tr}x{cH_tr} | (original): {oW_tr}x{oH_tr}\")\n",
    "print(\"--- Cleaning report ---\")\n",
    "print(f\"STRICT={STRICT}, UNKNOWN_POLICY='{UNKNOWN_POLICY}'\")\n",
    "print(\"Note: counts reflect both corrupted/original files across all folders.\")\n",
    "# We can’t read the `stats` dict returned from a generator after exhaustion; re-scan quickly to print stats.\n",
    "# Light-weight rescan to only count issues:\n",
    "def quick_scan_unknowns(base: Path) -> Dict[str,int]:\n",
    "    s = {\"folders_seen\":0,\"skipped_missing_files\":0,\"skipped_comment_lines\":0,\n",
    "         \"skipped_separator_lines\":0,\"unknown_chars\":0,\"dropped_chars\":0,\"dropped_empty_after_clean\":0}\n",
    "    for sub in sorted([p for p in base.iterdir() if p.is_dir()]):\n",
    "        s[\"folders_seen\"] += 1\n",
    "        corr = sub / \"corrupted.txt\"\n",
    "        orig = sub / \"original.txt\"\n",
    "        if not (corr.exists() and orig.exists()):\n",
    "            s[\"skipped_missing_files\"] += 1\n",
    "            continue\n",
    "        for p in (corr, orig):\n",
    "            lines = read_level_txt(p)\n",
    "            # simulate sanitize (no errors)\n",
    "            _stats = {k:0 for k in s.keys()}\n",
    "            sanitize_lines(lines, _stats)\n",
    "            for k in s.keys():\n",
    "                if k in _stats: s[k] += _stats[k]\n",
    "    return s\n",
    "\n",
    "scan = quick_scan_unknowns(DATA_DIR)\n",
    "for k,v in scan.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "print(\"If '-' or other stray characters are meaningful tiles for you, tell me and I'll add them to VOCAB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f145909-39f2-4f07-867a-3e276e24ff21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train.jsonl] Dropped 1 samples exceeding MAX_LEN=4096\n",
      "Loaded: train=3073 | val=384 | vocab=35 (+PAD)\n"
     ]
    }
   ],
   "source": [
    "# # Cell: Fine-tune a small Transformer to repair levels (corrupted_ids -> original_ids)\n",
    "\n",
    "# import json, math, random\n",
    "# from pathlib import Path\n",
    "# from typing import List, Dict, Tuple\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# # ----------------------------\n",
    "# # Paths & config\n",
    "# # ----------------------------\n",
    "# DATA_DIR = Path(\"processed\")\n",
    "# TRAIN_PATH = DATA_DIR / \"train.jsonl\"\n",
    "# VAL_PATH   = DATA_DIR / \"val.jsonl\"\n",
    "# VOCAB_PATH = DATA_DIR / \"vocab.json\"\n",
    "\n",
    "# SAVE_DIR   = Path(\"checkpoints\")\n",
    "# SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# CKPT_PATH  = SAVE_DIR / \"level_repair_transformer.pt\"\n",
    "# TOKCONF    = SAVE_DIR / \"token_config.json\"\n",
    "\n",
    "# SEED = 42\n",
    "# random.seed(SEED)\n",
    "# torch.manual_seed(SEED)\n",
    "\n",
    "# # Model/training hyperparams (tweak as needed)\n",
    "# BATCH_SIZE = 32\n",
    "# EMBED_DIM  = 256\n",
    "# FF_DIM     = 512\n",
    "# N_HEADS    = 8\n",
    "# N_LAYERS   = 4\n",
    "# DROPOUT    = 0.1\n",
    "# LR         = 3e-4\n",
    "# EPOCHS     = 5\n",
    "# MAX_LEN    = 4096          # maximum allowed sequence length after flattening\n",
    "\n",
    "# # ----------------------------\n",
    "# # Load vocab and define PAD token\n",
    "# # ----------------------------\n",
    "# with VOCAB_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "#     vocab_file = json.load(f)\n",
    "# VOCAB = vocab_file[\"vocab\"]                   # list of string tokens\n",
    "# tok2id = vocab_file[\"tok2id\"]                 # mapping str -> int\n",
    "\n",
    "# # add a dedicated PAD token id for sequence padding (not present in the training data)\n",
    "# PAD_ID = len(VOCAB)\n",
    "# NUM_TOKENS = len(VOCAB) + 1                   # +1 for PAD\n",
    "\n",
    "# # Save token config used by the model\n",
    "# with TOKCONF.open(\"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump({\"PAD_ID\": PAD_ID,\n",
    "#                \"NUM_TOKENS\": NUM_TOKENS,\n",
    "#                \"VOCAB\": VOCAB}, f, indent=2)\n",
    "\n",
    "# # ----------------------------\n",
    "# # Dataset\n",
    "# # ----------------------------\n",
    "# def read_jsonl(path: Path) -> List[Dict]:\n",
    "#     rows = []\n",
    "#     with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "#         for ln in f:\n",
    "#             if ln.strip():\n",
    "#                 rows.append(json.loads(ln))\n",
    "#     return rows\n",
    "\n",
    "# class LevelPairs(Dataset):\n",
    "#     def __init__(self, jsonl_path: Path, max_len: int = MAX_LEN):\n",
    "#         self.rows = read_jsonl(jsonl_path)\n",
    "#         self.max_len = max_len\n",
    "\n",
    "#         # Optionally filter overly long samples (Transformer memory safeguard)\n",
    "#         kept = []\n",
    "#         for r in self.rows:\n",
    "#             if len(r[\"corrupted_ids\"]) <= self.max_len and len(r[\"original_ids\"]) <= self.max_len:\n",
    "#                 kept.append(r)\n",
    "#         dropped = len(self.rows) - len(kept)\n",
    "#         self.rows = kept\n",
    "#         if dropped:\n",
    "#             print(f\"[{jsonl_path.name}] Dropped {dropped} samples exceeding MAX_LEN={self.max_len}\")\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.rows)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         r = self.rows[idx]\n",
    "#         src = r[\"corrupted_ids\"]\n",
    "#         tgt = r[\"original_ids\"]\n",
    "#         # We feed tgt_in (shifted right, BOS-free) and compute loss on tgt_out (shifted left)\n",
    "#         # Here we skip using explicit BOS/EOS; we learn to reproduce full sequence.\n",
    "#         return torch.tensor(src, dtype=torch.long), torch.tensor(tgt, dtype=torch.long)\n",
    "\n",
    "# def collate_fn(batch: List[Tuple[torch.Tensor, torch.Tensor]]):\n",
    "#     # Pad to max length in batch with PAD_ID\n",
    "#     src_seqs, tgt_seqs = zip(*batch)\n",
    "#     max_src = max(s.size(0) for s in src_seqs)\n",
    "#     max_tgt = max(t.size(0) for t in tgt_seqs)\n",
    "\n",
    "#     padded_src = torch.full((len(batch), max_src), PAD_ID, dtype=torch.long)\n",
    "#     padded_tin = torch.full((len(batch), max_tgt), PAD_ID, dtype=torch.long)  # decoder input\n",
    "#     padded_tout= torch.full((len(batch), max_tgt), PAD_ID, dtype=torch.long)  # supervised target\n",
    "\n",
    "#     for i, (s, t) in enumerate(zip(src_seqs, tgt_seqs)):\n",
    "#         # decoder: teacher forcing with 1-position shift (tin = [t0..t_{n-2}, t_{n-1}], tout = [t0..t_{n-1}])\n",
    "#         padded_src[i, :s.size(0)] = s\n",
    "#         padded_tin[i, :t.size(0)] = t\n",
    "#         padded_tout[i,:t.size(0)] = t\n",
    "\n",
    "#     src_key_padding_mask = (padded_src == PAD_ID)  # (B, S)\n",
    "#     tgt_key_padding_mask = (padded_tin == PAD_ID)  # (B, T)\n",
    "\n",
    "#     return {\n",
    "#         \"src\": padded_src,\n",
    "#         \"tin\": padded_tin,\n",
    "#         \"tout\": padded_tout,\n",
    "#         \"src_mask\": src_key_padding_mask,\n",
    "#         \"tgt_mask\": tgt_key_padding_mask,\n",
    "#     }\n",
    "\n",
    "# train_ds = LevelPairs(TRAIN_PATH)\n",
    "# val_ds   = LevelPairs(VAL_PATH)\n",
    "\n",
    "# train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn, num_workers=0)\n",
    "# val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "# print(f\"Loaded: train={len(train_ds)} | val={len(val_ds)} | vocab={len(VOCAB)} (+PAD)\")\n",
    "\n",
    "# # ----------------------------\n",
    "# # Model\n",
    "# # ----------------------------\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model: int, max_len: int = 65536):\n",
    "#         super().__init__()\n",
    "#         pe = torch.zeros(max_len, d_model)\n",
    "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
    "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#         self.register_buffer('pe', pe.unsqueeze(0))  # (1, L, D)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: (B, L, D)\n",
    "#         L = x.size(1)\n",
    "#         return x + self.pe[:, :L, :]\n",
    "\n",
    "# class RepairTransformer(nn.Module):\n",
    "#     def __init__(self, vocab_size: int, pad_id: int, d_model: int, ff_dim: int,\n",
    "#                  n_heads: int, n_layers: int, dropout: float):\n",
    "#         super().__init__()\n",
    "#         self.pad_id = pad_id\n",
    "#         self.d_model = d_model\n",
    "\n",
    "#         self.src_emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "#         self.tgt_emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "#         self.pos_enc = PositionalEncoding(d_model)\n",
    "\n",
    "#         encoder_layer = nn.TransformerEncoderLayer(d_model, n_heads, ff_dim, dropout, batch_first=True)\n",
    "#         decoder_layer = nn.TransformerDecoderLayer(d_model, n_heads, ff_dim, dropout, batch_first=True)\n",
    "#         self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "#         self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=n_layers)\n",
    "\n",
    "#         self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "#     def make_square_subsequent_mask(self, size: int, device):\n",
    "#         # causal mask for decoder (T, T)\n",
    "#         return torch.triu(torch.full((size, size), float('-inf'), device=device), diagonal=1)\n",
    "\n",
    "#     def forward(self, src, tin, src_pad_mask, tgt_pad_mask):\n",
    "#         # src, tin: (B, S/T)\n",
    "#         src_emb = self.pos_enc(self.src_emb(src))\n",
    "#         tgt_emb = self.pos_enc(self.tgt_emb(tin))\n",
    "\n",
    "#         memory = self.encoder(src_emb, src_key_padding_mask=src_pad_mask)  # (B, S, D)\n",
    "\n",
    "#         # causal mask for decoder self-attn\n",
    "#         T = tin.size(1)\n",
    "#         causal = self.make_square_subsequent_mask(T, tin.device)\n",
    "#         out = self.decoder(\n",
    "#             tgt_emb, memory,\n",
    "#             tgt_mask=causal,\n",
    "#             tgt_key_padding_mask=tgt_pad_mask,\n",
    "#             memory_key_padding_mask=src_pad_mask\n",
    "#         )  # (B, T, D)\n",
    "#         logits = self.proj(out)  # (B, T, V)\n",
    "#         return logits\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = RepairTransformer(\n",
    "#     vocab_size=NUM_TOKENS, pad_id=PAD_ID,\n",
    "#     d_model=EMBED_DIM, ff_dim=FF_DIM,\n",
    "#     n_heads=N_HEADS, n_layers=N_LAYERS, dropout=DROPOUT\n",
    "# ).to(device)\n",
    "\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "# criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "\n",
    "# # ----------------------------\n",
    "# # Train / eval loops\n",
    "# # ----------------------------\n",
    "# def run_epoch(loader, train_mode=True):\n",
    "#     model.train(train_mode)\n",
    "#     total_loss = 0.0\n",
    "#     total_tokens = 0\n",
    "#     for batch in loader:\n",
    "#         src  = batch[\"src\"].to(device)\n",
    "#         tin  = batch[\"tin\"].to(device)\n",
    "#         tout = batch[\"tout\"].to(device)\n",
    "#         src_mask = batch[\"src_mask\"].to(device)\n",
    "#         tgt_mask = batch[\"tgt_mask\"].to(device)\n",
    "\n",
    "#         if train_mode:\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#         logits = model(src, tin, src_mask, tgt_mask)  # (B, T, V)\n",
    "#         # Compute loss over all positions\n",
    "#         loss = criterion(logits.reshape(-1, logits.size(-1)), tout.reshape(-1))\n",
    "\n",
    "#         if train_mode:\n",
    "#             loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#             optimizer.step()\n",
    "\n",
    "#         # token count excludes pads in target\n",
    "#         ntoks = (tout != PAD_ID).sum().item()\n",
    "#         total_loss += loss.item() * ntoks\n",
    "#         total_tokens += ntoks\n",
    "\n",
    "#     return total_loss / max(1, total_tokens)\n",
    "\n",
    "# best_val = float(\"inf\")\n",
    "# for epoch in range(1, EPOCHS + 1):\n",
    "#     tr_loss = run_epoch(train_dl, train_mode=True)\n",
    "#     vl_loss = run_epoch(val_dl, train_mode=False)\n",
    "#     print(f\"Epoch {epoch:02d} | train xent: {tr_loss:.4f} | val xent: {vl_loss:.4f} | ppl: {math.exp(min(vl_loss, 20)):.2f}\")\n",
    "\n",
    "#     if vl_loss < best_val:\n",
    "#         best_val = vl_loss\n",
    "#         torch.save({\n",
    "#             \"model_state_dict\": model.state_dict(),\n",
    "#             \"config\": {\n",
    "#                 \"EMBED_DIM\": EMBED_DIM, \"FF_DIM\": FF_DIM,\n",
    "#                 \"N_HEADS\": N_HEADS, \"N_LAYERS\": N_LAYERS,\n",
    "#                 \"DROPOUT\": DROPOUT, \"PAD_ID\": PAD_ID,\n",
    "#                 \"NUM_TOKENS\": NUM_TOKENS\n",
    "#             }\n",
    "#         }, CKPT_PATH)\n",
    "#         print(f\"  ✓ Saved checkpoint -> {CKPT_PATH}\")\n",
    "\n",
    "# # ----------------------------\n",
    "# # Greedy decode on a few val samples\n",
    "# # ----------------------------\n",
    "# id2tok = {int(v): k for k, v in tok2id.items()}\n",
    "# id2tok[PAD_ID] = \"<PAD>\"\n",
    "\n",
    "# def greedy_decode(src_ids: List[int], max_len: int = 4096) -> List[int]:\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         src = torch.tensor(src_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "#         src_mask = (src == PAD_ID)\n",
    "#         memory = model.encoder(model.pos_enc(model.src_emb(src)), src_key_padding_mask=src_mask)\n",
    "\n",
    "#         # initialize decoder input with zeros — here we’ll just start from a copy prompt\n",
    "#         # (you can introduce BOS/EOS later if you want exact length control)\n",
    "#         out = torch.full((1, 1), PAD_ID, dtype=torch.long, device=device)\n",
    "\n",
    "#         for _ in range(min(len(src_ids), max_len)):\n",
    "#             tgt_mask = (out == PAD_ID)\n",
    "#             causal = model.make_square_subsequent_mask(out.size(1), device)\n",
    "#             dec = model.decoder(\n",
    "#                 model.pos_enc(model.tgt_emb(out)),\n",
    "#                 memory, tgt_mask=causal,\n",
    "#                 tgt_key_padding_mask=tgt_mask,\n",
    "#                 memory_key_padding_mask=src_mask\n",
    "#             )\n",
    "#             logits = model.proj(dec[:, -1, :])  # (1, V)\n",
    "#             next_id = logits.argmax(-1).unsqueeze(1)  # (1,1)\n",
    "#             out = torch.cat([out, next_id], dim=1)\n",
    "#         # drop the first PAD seed token\n",
    "#         return out.squeeze(0).tolist()[1:len(src_ids)+1]\n",
    "\n",
    "# def ids_to_grid(ids: List[int], width: int, height: int) -> List[str]:\n",
    "#     # reconstruct rows using your BACKGROUND padding policy; assumes rectangular dims were saved earlier\n",
    "#     chars = []\n",
    "#     for i in ids:\n",
    "#         if i == PAD_ID:\n",
    "#             ch = \"|\"  # visualize PAD as background, you can pick something else\n",
    "#         else:\n",
    "#             ch = VOCAB[i]\n",
    "#         chars.append(ch)\n",
    "#     # clamp if lengths mismatch\n",
    "#     total = width * height\n",
    "#     chars = chars[:total] + ([\"|\"] * max(0, total - len(chars)))\n",
    "#     return [\"\".join(chars[r*width:(r+1)*width]) for r in range(height)]\n",
    "\n",
    "# # Show a few qualitative examples from val set\n",
    "# val_rows = read_jsonl(VAL_PATH)\n",
    "# print(\"\\n=== Qualitative samples (val) ===\")\n",
    "# for r in random.sample(val_rows, k=min(3, len(val_rows))):\n",
    "#     src = r[\"corrupted_ids\"]\n",
    "#     tgt = r[\"original_ids\"]\n",
    "#     w   = r.get(\"width_original\", 0) or r.get(\"width_corrupted\", 0)\n",
    "#     h   = r.get(\"height_original\", 0) or r.get(\"height_corrupted\", 0)\n",
    "\n",
    "#     pred = greedy_decode(src, max_len=len(src))\n",
    "#     # decode to grid strings\n",
    "#     tgt_grid  = ids_to_grid(tgt, w, h) if (w and h) else [\"(no dims recorded)\"]\n",
    "#     pred_grid = ids_to_grid(pred, w, h) if (w and h) else [\"(no dims recorded)\"]\n",
    "\n",
    "#     print(f\"\\nLevel: {r.get('level_id','?')}\")\n",
    "#     print(\"Corrupted (first 120 ids):\", src[:120], \"...\")\n",
    "#     print(\"Target    (first 120 ids):\", tgt[:120], \"...\")\n",
    "#     print(\"Pred      (first 120 ids):\", pred[:120], \"...\")\n",
    "#     if w and h:\n",
    "#         print(f\"Dims: {w}x{h}\")\n",
    "#         print(\"Target grid preview:\")\n",
    "#         for line in tgt_grid[:min(6, len(tgt_grid))]:\n",
    "#             print(\" \", line[:min(120, len(line))])\n",
    "#         print(\"Pred grid preview:\")\n",
    "#         for line in pred_grid[:min(6, len(pred_grid))]:\n",
    "#             print(\" \", line[:min(120, len(line))])\n",
    "\n",
    "# print(f\"\\nDone. Best checkpoint at: {CKPT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb89ed8-d8cd-403c-b620-a24e05729246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Fine-tune a Hugging Face causal LM (Qwen / gpt-oss / GPT-2) for level repair\n",
    "# !pip install -q transformers accelerate datasets\n",
    "\n",
    "import json, random, math\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "DATA_DIR   = Path(\"processed\")\n",
    "TRAIN_PATH = DATA_DIR / \"train.jsonl\"\n",
    "VAL_PATH   = DATA_DIR / \"val.jsonl\"\n",
    "VOCAB_PATH = DATA_DIR / \"vocab.json\"\n",
    "\n",
    "# Choose a small-ish causal LM; you can swap:\n",
    "#   - \"Qwen/Qwen2.5-0.5B\" (good quality; needs GPU)\n",
    "#   - \"Qwen/Qwen2-0.5B\"\n",
    "#   - \"Gryphe/gpt-oss-mini\" or \"Gryphe/gpt-oss-125M\" (community)\n",
    "#   - \"gpt2\" (tiny, CPU-friendly baseline)\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"   # change if needed\n",
    "OUTPUT_DIR = \"hf-checkpoints/level-repair-qwen\"\n",
    "SEED       = 42\n",
    "\n",
    "BATCH_SIZE = 2          # keep small for big models; increase if you have VRAM\n",
    "GRAD_ACCUM = 8\n",
    "EPOCHS     = 3\n",
    "LR         = 2e-5\n",
    "MAX_LEN    = 4096       # max tokens per example after tokenization\n",
    "USE_ROW_BREAKS = True   # if dims saved, insert \"\\n\" at row breaks for structure\n",
    "PREVIEW_SAMPLES = 3\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# Load vocab (id -> tile char)\n",
    "# -----------------------------\n",
    "with open(VOCAB_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    vconf = json.load(f)\n",
    "VOCAB: List[str] = vconf[\"vocab\"]\n",
    "TOK2ID: Dict[str,int] = {k:int(v) for k,v in vconf[\"tok2id\"].items()}\n",
    "ID2TOK: Dict[int,str] = {v:k for k,v in TOK2ID.items()}\n",
    "\n",
    "def ids_to_grid(ids: List[int], w: int, h: int) -> str:\n",
    "    \"\"\"Rebuild a grid string with optional row breaks for better structure learning.\"\"\"\n",
    "    chars = [ID2TOK.get(i, \"|\") for i in ids]\n",
    "    if USE_ROW_BREAKS and w and h and w*h <= len(chars) + 64:  # tolerate a little mismatch\n",
    "        chars = chars[:w*h]\n",
    "        rows = [\"\".join(chars[r*w:(r+1)*w]) for r in range(h)]\n",
    "        return \"\\n\".join(rows)\n",
    "    else:\n",
    "        return \"\".join(chars)\n",
    "\n",
    "def read_jsonl(path: Path) -> List[Dict]:\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for ln in f:\n",
    "            if ln.strip():\n",
    "                rows.append(json.loads(ln))\n",
    "    return rows\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenizer & model\n",
    "# -----------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "# Ensure we have a pad token (some decoder-only models don't). Use eos as pad if needed.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "# Align model embeddings with tokenizer size if necessary (usually not needed for stock tokenizer)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "INSTR = \"### Instruction:\\nRepair the level.\\n\\n\"\n",
    "CORR  = \"### Corrupted:\\n\"\n",
    "REPR  = \"\\n\\n### Repaired:\\n\"\n",
    "\n",
    "def build_example(row: Dict) -> Dict[str, torch.Tensor]:\n",
    "    # Build text fields\n",
    "    w = row.get(\"width_original\") or row.get(\"width_corrupted\") or 0\n",
    "    h = row.get(\"height_original\") or row.get(\"height_corrupted\") or 0\n",
    "    corrupted_txt = ids_to_grid(row[\"corrupted_ids\"], w, h)\n",
    "    target_txt    = ids_to_grid(row[\"original_ids\"],  w, h)\n",
    "\n",
    "    prompt = INSTR + CORR + corrupted_txt + REPR\n",
    "    full   = prompt + target_txt\n",
    "\n",
    "    enc = tokenizer(full, truncation=True, max_length=MAX_LEN, padding=False, return_tensors=None)\n",
    "    # Compute label mask: loss only on target portion\n",
    "    p_ids = tokenizer(prompt, truncation=True, max_length=MAX_LEN, padding=False, return_tensors=None)[\"input_ids\"]\n",
    "    labels = enc[\"input_ids\"][:]\n",
    "    loss_mask_upto = len(p_ids)  # positions before this should be ignored (-100)\n",
    "\n",
    "    labels = [-100 if i < loss_mask_upto else tok for i, tok in enumerate(labels)]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(enc[\"input_ids\"], dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(enc[\"attention_mask\"], dtype=torch.long),\n",
    "        \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        \"level_id\": row.get(\"level_id\", \"\")\n",
    "    }\n",
    "\n",
    "class RepairDataset(Dataset):\n",
    "    def __init__(self, jsonl_path: Path):\n",
    "        raw = read_jsonl(jsonl_path)\n",
    "        # Optional: filter pathological long pairs after tokenization\n",
    "        processed = []\n",
    "        for r in raw:\n",
    "            ex = build_example(r)\n",
    "            if ex[\"input_ids\"].numel() <= MAX_LEN:\n",
    "                processed.append(ex)\n",
    "        dropped = len(raw) - len(processed)\n",
    "        if dropped:\n",
    "            print(f\"[{jsonl_path.name}] dropped {dropped} over-long samples (> {MAX_LEN} tokens)\")\n",
    "        self.data = processed\n",
    "\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx): return self.data[idx]\n",
    "\n",
    "train_ds = RepairDataset(TRAIN_PATH)\n",
    "val_ds   = RepairDataset(VAL_PATH)\n",
    "print(f\"Loaded HF datasets: train={len(train_ds)} | val={len(val_ds)} | model={MODEL_NAME}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Data collator (already pre-tokenized & padded per-batch by Trainer)\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class SimpleCollator:\n",
    "    pad_token_id: int\n",
    "    def __call__(self, features):\n",
    "        # Dynamic pad to the longest in batch\n",
    "        batch = {}\n",
    "        keys = [\"input_ids\",\"attention_mask\",\"labels\"]\n",
    "        max_len = max(len(f[\"input_ids\"]) for f in features)\n",
    "        for k in keys:\n",
    "            padded = []\n",
    "            for f in features:\n",
    "                seq = f[k]\n",
    "                pad_id = self.pad_token_id if k != \"labels\" else -100\n",
    "                if len(seq) < max_len:\n",
    "                    pad_len = max_len - len(seq)\n",
    "                    if isinstance(seq, torch.Tensor):\n",
    "                        seq = seq.tolist()\n",
    "                else:\n",
    "                    pad_len = 0\n",
    "                    if isinstance(seq, torch.Tensor):\n",
    "                        seq = seq.tolist()\n",
    "                seq = seq + [pad_id]*pad_len\n",
    "                padded.append(torch.tensor(seq, dtype=torch.long))\n",
    "            batch[k] = torch.stack(padded, dim=0)\n",
    "        return batch\n",
    "\n",
    "collator = SimpleCollator(pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "# -----------------------------\n",
    "# Training\n",
    "# -----------------------------\n",
    "# args = TrainingArguments(\n",
    "#     output_dir=OUTPUT_DIR,\n",
    "#     per_device_train_batch_size=BATCH_SIZE,\n",
    "#     per_device_eval_batch_size=BATCH_SIZE,\n",
    "#     gradient_accumulation_steps=GRAD_ACCUM,\n",
    "#     num_train_epochs=EPOCHS,\n",
    "#     learning_rate=LR,\n",
    "#     lr_scheduler_type=\"cosine\",\n",
    "#     warmup_ratio=0.03,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_steps=25,\n",
    "#     evaluation_strategy=\"steps\",\n",
    "#     eval_steps=100,\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=100,\n",
    "#     save_total_limit=2,\n",
    "#     bf16=torch.cuda.is_available(),   # use bf16 when possible\n",
    "#     fp16=not torch.cuda.is_available() and False,  # keep False on CPU\n",
    "#     report_to=\"none\",\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     tokenizer=tokenizer,\n",
    "#     data_collator=collator,\n",
    "#     train_dataset=train_ds,\n",
    "#     eval_dataset=val_ds,\n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "# trainer.save_model(OUTPUT_DIR)  # saves adapter/weights + tokenizer\n",
    "\n",
    "# # -----------------------------\n",
    "# # Quick qualitative check\n",
    "# # -----------------------------\n",
    "# def generate_repair(corrupted_ids: List[int], w:int, h:int, max_new_tokens:int=4096) -> str:\n",
    "#     corrupted_txt = ids_to_grid(corrupted_ids, w, h)\n",
    "#     prompt = INSTR + CORR + corrupted_txt + REPR\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "#     with torch.no_grad():\n",
    "#         out = model.generate(\n",
    "#             **inputs,\n",
    "#             max_new_tokens=min(max_new_tokens, 2*w*h if w and h else 2048),\n",
    "#             do_sample=False,\n",
    "#             temperature=1.0,\n",
    "#             top_p=1.0,\n",
    "#             eos_token_id=tokenizer.eos_token_id,\n",
    "#             pad_token_id=tokenizer.pad_token_id,\n",
    "#         )\n",
    "#     text = tokenizer.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "#     return text\n",
    "\n",
    "# val_rows = read_jsonl(VAL_PATH)\n",
    "# print(\"\\n=== Generations (validation samples) ===\")\n",
    "# for row in random.sample(val_rows, k=min(PREVIEW_SAMPLES, len(val_rows))):\n",
    "#     w = row.get(\"width_original\") or row.get(\"width_corrupted\") or 0\n",
    "#     h = row.get(\"height_original\") or row.get(\"height_corrupted\") or 0\n",
    "#     pred = generate_repair(row[\"corrupted_ids\"], w, h)\n",
    "#     print(f\"\\nLevel: {row.get('level_id','?')}\")\n",
    "#     print(\"Pred repaired (first 6 lines):\")\n",
    "#     print(\"\\n\".join(pred.splitlines()[:6]))\n",
    "\n",
    "\n",
    "# Cell: Ultra-compatible Trainer patch (filters kwargs to match your transformers version)\n",
    "import inspect, math, torch, transformers\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "print(\"Transformers version:\", transformers.__version__)\n",
    "\n",
    "def filter_kwargs(cls, kwargs):\n",
    "    params = set(inspect.signature(cls.__init__).parameters.keys())\n",
    "    return {k: v for k, v in kwargs.items() if k in params}\n",
    "\n",
    "# Start with a generous set, then filter to what your version supports\n",
    "train_args = dict(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=25,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    lr_scheduler_type=\"cosine\",   # will be dropped if unsupported\n",
    "    warmup_ratio=0.03,            # will be dropped if unsupported\n",
    "    bf16=torch.cuda.is_available(),  # will be dropped if unsupported\n",
    "    fp16=(torch.cuda.is_available() and not torch.cuda.is_available()),  # will be dropped if unsupported\n",
    "    report_to=\"none\",             # will be dropped if unsupported\n",
    "    # NOTE: we intentionally DO NOT pass evaluation_strategy or evaluate_during_training\n",
    ")\n",
    "\n",
    "args = TrainingArguments(**filter_kwargs(TrainingArguments, train_args))\n",
    "\n",
    "# Some ancient Trainer versions don't accept tokenizer in ctor; guard it too.\n",
    "trainer_kwargs = dict(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,  # will be filtered below if unsupported\n",
    ")\n",
    "\n",
    "if \"tokenizer\" not in inspect.signature(Trainer.__init__).parameters:\n",
    "    trainer_kwargs.pop(\"tokenizer\", None)\n",
    "\n",
    "trainer = Trainer(**trainer_kwargs)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Manual eval (since scheduling args aren’t available)\n",
    "try:\n",
    "    eval_out = trainer.evaluate()\n",
    "    print(\"Eval metrics:\", eval_out)\n",
    "    if \"eval_loss\" in eval_out:\n",
    "        ppl = math.exp(min(20, eval_out[\"eval_loss\"]))\n",
    "        print(f\"Perplexity (clamped): {ppl:.2f}\")\n",
    "except Exception as e:\n",
    "    print(\"Evaluation skipped (not supported in this version):\", e)\n",
    "\n",
    "# Save model\n",
    "try:\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    print(\"Saved to:\", OUTPUT_DIR)\n",
    "except Exception as e:\n",
    "    print(\"Save failed:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rp_mario)",
   "language": "python",
   "name": "rp_mario"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
